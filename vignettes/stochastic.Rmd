---
title: "Stochastic Feedforward Network"
author: "Andreas M. Brandmaier"
date: "SEP/03/2020"
output: pdf_document
#vignette: >
#  %\VignetteIndexEntry{Vignette Title}
#  %\VignetteEngine{knitr::rmarkdown}
#  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

dyn.load(paste("../bnnlib", .Platform$dynlib.ext, sep=""))
source("../bnnlib.R")
cacheMetaData(1)
```

## Feedforward Neural Network

Create a feed-forward neural network with 3 inputs, 25 hidden units, and 1 output. The output node is linear.

```{r}
LINEAR_NODE = 3

gain = 1
bias = 0
stochastic_range=0

in_size = 3

net <- NetworkFactory_createStochasticFeedForwardNetwork(in_size,25,1, LINEAR_NODE,
                                                         gain, bias, stochastic_range)
```

Load the marketing dataset from the datarium package.


```{r}
data("marketing", package = "datarium")
head(marketing, 4)

```

Convert data to Sequence format
```{r}
sequence_set <- SequenceSet()

source("../R/toSequence.R")
seq <- toSequence(marketing, 1:3, 4)
SequenceSet_add_copy_of_sequence(sequence_set,seq)

cat("Sequences created: ",SequenceSet_size(sequence_set),"\n")
cat("Input size",SequenceSet_input_size_get(sequence_set),"\n")
cat("Target size",SequenceSet_target_size_get(sequence_set),"\n")

```

Initialize training algorithm

```{r}
bp <- ImprovedRPropTrainer(net)

cat("Using Trainer: ",Trainer_get_name(bp),"\n")
```

Start training

```{r}
iterations <- 100
steps.per.iteration <- 1
err <- rep(NA, iterations+1)
err[1] <- Network_evaluate_training_error__SWIG_0(net, sequence_set)
for (i in 1:iterations) {
  Trainer_train__SWIG_0(bp, sequence_set, steps.per.iteration)
  err[i+1] <- Network_evaluate_training_error__SWIG_0(net, sequence_set)
}
```

With ggplot2, we can plot the training set error over iterations:

```{r warning=FALSE, message=FALSE}
require(ggplot2)
ggplot(data=data.frame(step=1:(iterations+1),err),aes(x=step,y=err))+
  geom_point()+ geom_line()+
  geom_smooth()+
  theme_minimal()
```

Inspect output:

```{r}

outputs <- getOutputs(net, seq)
targets <- getTargets(seq)
ggplot(data=data.frame(outputs=outputs, targets=targets), aes(y=outputs,x=targets))+
  geom_point()+
  theme_minimal()+
  xlab("Targets")+ylab("Predictions")+
  geom_smooth()+
  ggtitle("Sales")

```
Next, we examine the difference in outputs when we run the same set of stimuli through the network. The stochasticity of the gain functions should yields slightly different results.
```{r}
#e = Network_get_ensemble(net, 1)
#StochasticFeedforwardEnsemble_set_stochastic_range(e, 0)


outputs2 <- getOutputs(net, seq)

plot(outputs[,1], outputs2[,1])
#act <- getActivations()

```