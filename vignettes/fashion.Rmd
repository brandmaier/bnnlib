---
title: "fashion"
author: "Andreas M. Brandmaier"
date: "10/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r message=FALSE}
library(bnnlib)
```

```{r cars}
library(snedata)

# fetch the Fashion-MNIST dataset
fashion <- download_fashion_mnist()

# Works as a drop-in replacement for the MNIST digits. Show a t-shirt and a shoe
show_mnist_digit(fashion, 2)
show_mnist_digit(fashion, 425)
```

## Convert data

```{r}

seqset = SequenceSet()

n_seq <- 1000

for (i in 1:n_seq) {
  seq = Sequence()
  input_vals <- simplify2array(fashion[i,1:784]/255)
  target_vals <- bnnlib:::onehot(fashion[i,785], alphabet = 0:9)
#  Sequence_add_from_array(seq, input_vals, target_vals, 784, 10)
  Sequence_add_from_array(seq, input_vals, input_vals, 784, 784)
  SequenceSet_add_copy_of_sequence(seqset, seq)
}

labels <-  fashion[1:n_seq, 785]
readable.labels <-  fashion[1:n_seq, 786]

label.set <- NA
label.set[as.numeric(as.character(labels))+1] <- as.character(readable.labels)
```


## Train a network

One thousand digits with 200 epochs take about 90s on a 2020 laptop.

```{r}
dims<-784

net_sparse <- AutoAssociationNetwork(dims, 20, bnnlib::TANH_NODE, bnnlib::SIGMOID_NODE, 0.1)

bp <- ImprovedRPropTrainer(net_sparse)
Trainer_sparsity_beta_set(bp, .0)
library(tictoc)
tic()
Trainer_train__SWIG_0(bp, seqset, 50)
toc()

library(ggplot2)
plotTrainingerror(bp) + scale_y_log10()
```


## Plot predictions

```{r}

plotPreds <- function(sequence, net, dim=28) {
  x = getOutputs(net, sequence )
  #x2 = sapply(1:(84*84), function(x){ Sequence_get_input_at(sequence,0,x-1)})
  x2 = getInputs(sequence)
  #getInputs()
  predimg <- matrix(nrow=dim, ncol=dim)
  inpimg <- matrix(nrow=dim, ncol=dim)
  for (i in 1:dim) {
    predimg[i,] <- x[1:dim+(i-1)*dim]
    inpimg[i,] <- x2[1:dim+(i-1)*dim]
  }
  
  par(mfrow=c(2,2))
  image(inpimg, main="Input")
  image(predimg, main="Prediction")
  image(predimg-inpimg, main="Difference")
  image(abs(predimg-inpimg),main="Absolute Difference")
}


plotPreds(SequenceSet_get(seqset,4), net_sparse)
plotPreds(SequenceSet_get(seqset,102), net_sparse)
plotPreds(SequenceSet_get(seqset,8), net_sparse)
plotPreds(SequenceSet_get(seqset,40), net_sparse)
plotPreds(SequenceSet_get(seqset,41), net_sparse)
plotPreds(SequenceSet_get(seqset,141), net_sparse)
plotPreds(SequenceSet_get(seqset,241), net_sparse)
```

## Plot latent space projection

```{r message=FALSE}


pos <- which(sapply(getNodeNames(net_sparse) , function(x){startsWith(x,"Bottleneck")}))

result <- matrix(nrow=0,ncol=length(pos))
for (i in 1:SequenceSet_size(seqset)) {
  sequence <- SequenceSet_get(seqset,i-1)
  x = getActivations(net_sparse, sequence)[1,pos]
  result <- rbind(result, x)
}

result <- data.frame(result)

cols <- rainbow(10)
par(mfrow=c(1,1))
if (sum(dist(result))!=0) {
  coords <- cmdscale(dist(result),2)
  plot(coords,type="n")
  text(x=coords[,1],y=coords[,2], labels=labels,col = cols[labels])
}
```

## Hidden Layer Average Representations

```{r}
par(mfrow=c(3,3))
sapply(1:9,  function(lb=1) {
  smm<-colMeans(result[labels==lb,])
  barplot(smm,main = paste0("Hidden Layer for ",label.set[lb]))
})

```

## Modify network

Create a second neural network that only has the decompression part of the auto-associator. That is, the hidden layer becomes the input layer of the new network.

```{r eval=TRUE}
net_decomp <- AutoAssociationNetwork_createDecompressionNetwork(net_sparse)
Network_in_size_get(net_decomp)
Network_out_size_get(net_decomp)
```

Create artificial inputs to investigate the geometric primitives represented by the hidden layer neurons
```{r eval=TRUE}

 # par(mfrow=c(4,3))
  sapply(1:10, function(x){
  seq = Sequence()
  input_vals <- rep(-1,10)
  input_vals[x]<-1
  #input_vals[3]<-0
  Sequence_add_from_array(seq, input_vals, input_vals, 784, 784)
  
  pred <- getOutputs(net_decomp, seq)
  #title(paste0("Hidden"))
image(matrix(pred, nrow=28,byrow = TRUE))
})


```