---
title: "Poet"
author: "Elisabeth Riha and Andreas M. Brandmaier"
date: "1/6/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

## Setup

First, we load the shared library, some packages, and extra R code.

```{r fireup, echo=TRUE}
library(bnnlib)
library(gridExtra)
library(tictoc)
library(gutenbergr)

```

# Poet

The goal of this exercise is to create a recurrent neural network that can learn structure in text. We will then use this network to generate a novel text sequence.

## Obtain Training Data

As training data, we use the English translation of the Bible as provided from the `qlcMatrix` package. 

```{r}
library(qlcMatrix)
data(bibles)
fulltext <- paste0(bibles$eng,collapse = " ")

text = strsplit(tolower(paste0(fulltext,collapse="")),"")[[1]]

text <- text[1:80000] # take the first 80k letters

```

# Get a 0-1-Matrix for a given Alphabet and Text.

Next, we transform the text into a 0-1 matrix (one-hot coding), so that we can feed the text into a neural network, in which each input node corresponds to a single letter.

```{r}
# Get a 0-1 Matrix for a given alphabet and text.

myletters <- c(letters, " ") # use English letters plus space

# function that looks up whether a letter from the alphabet is in x, gives back True or False 
where_in_alphabet_ <- function(x, alphabet = myletters) {alphabet %in% x}

# 0-1 Matrix with option to return True-False Matrix when setting numeric = FALSE 
where_in_alphabet <- function(x, alphabet = letters, numeric = TRUE){
  out <- t(apply(matrix(x), 1, where_in_alphabet_, alphabet = alphabet))
  colnames(out) <- alphabet
  rownames(out) <- x
  if (numeric) out <- out*1
  out
}

tic()
tfmatrix = where_in_alphabet(text, myletters)
toc()

head(tfmatrix)

```


```{r}



tic()
seqset = SequenceSet()
seq = Sequence()

ln <- ncol(tfmatrix)

for (i in 1:(min(nrow(tfmatrix)-1,80000))  ) {
  input_vals = tfmatrix[i,]
  target_vals = tfmatrix[i+1,]
  
 Sequence_add_from_array(seq, input_vals, target_vals, ln, ln)
 
 if (i %% 100==0) {
   SequenceSet_add_copy_of_sequence(seqset, seq)
   seq = Sequence()
 }
}
  
 
 toc()
```

The following code creates a standard recurrent neural network with a winner-takes-all output layer. This layer enforces all outputs nodes to have activations larger or equal to zero while the sum of the activations equals 1. Thus, the output layer can be taken to represent a discrete probability distribution over letters.

```{r network}
network = NetworkFactory_createRecurrentWTANetwork(in_size=ln, hid_type=bnnlib::TANH_NODE, 
                                                   num_layers=1,
                                    layer_sizes=c(30,30),  out_size=ln);

Network_error_function_set(network, WinnerTakesAllErrorFunction())
```

## Auto-predict

A function to generate text from the network:

```{r autopred}

predict.text <- function(context=50, probabilistic=TRUE, time_steps = 400) {

sequence = SequenceSet_get(seqset,0)




if (!probabilistic) {
  predictor = AutoPredictor__SWIG_2(network, WinnerTakesAllTransferFunction() )
} else {
  predictor = AutoPredictor__SWIG_2(network, ProbabilisticWinnerTakesAllTransferFunction() )
}


x = AutoPredictor_predict( predictor, sequence, time_steps, context)
vals <- sapply(1:time_steps, FUN = function(index){getRow(x, index-1)})


generate_text2 <- function(tfmatrix, alphabet = letters) {
  outputvector <- vector(mode = "character", length = nrow(tfmatrix))
  for (i in seq_len(NROW(tfmatrix))) {
    replace <- alphabet[which.max(tfmatrix[i, ])]
    if(length(replace) == 0L) replace <- NA_character_ #control for case of numbers, other non-alphabetic elements
    outputvector[i] <- replace
  }
  outputvector
}

paste0(generate_text2(t(vals), alphabet=myletters),collapse = "")

}
```


Since the network only has random initial weights, the text will be random:

```{r linewidth=60}
predict.text()
```

Initialize the improved resilient propagation trainer.

```{r}
trainer = ImprovedRPropTrainer(network);
```

Train the network. This takes about 7 seconds with 40 hidden units and about 2 minutes for 140 hidden units per training iteration on an average computer of the year 2019.

```{r training}

tic()
Trainer_train2(trainer, seqset, 100)
toc()

```

Plot the traininer error

```{r message=FALSE}
	values <- getTrainingError(trainer)
	
	library(ggplot2)
	ggplot2::ggplot(data.frame(x=1:length(values),values),aes(x=x,y=values))+geom_point()+
	  geom_line()+geom_smooth()
```

## Generate new sequences

```{r}
chop <- function(txt) { sapply(seq(1,nchar(txt),30),function(x){ substr(txt,x,x+30)})}
```

Generating new text from the network using probabilistic and MAP prediction:

```{r predtext, linewidth=60}
print(predict.text(50,probabilistic = FALSE))
```

Now, let's use some probabilistic prediction that avoids getting stuck in highly likely loops.

```{r predtext2, linewidth=60}
print(predict.text(50,probabilistic = TRUE))
```

## Inspect network predictions

```{r inspection}
sequence = SequenceSet_get(seqset,0)
outputs = getOutputs(network, sequence)
barplot(outputs[1,],names.arg = myletters)
barplot(outputs[50,],names.arg = myletters)
````