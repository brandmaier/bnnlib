---
title: "Poet"
author: "Elisabeth Riha and Andreas M. Brandmaier"
date: "1/6/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

First, we load the shared library, some packages, and extra R code.

```{r fireup, echo=TRUE}

dyn.load(paste("../bnnlib", .Platform$dynlib.ext, sep=""))
source("../bnnlib.R")
cacheMetaData(1)

library(gridExtra)
library(tictoc)
library(gutenbergr)

source("../R/toSequence.R")
```


## Generate Data

```{r}
library(qlcMatrix)
data(bibles)
fulltext <- paste0(bibles$eng,collapse = " ")

text = strsplit(tolower(paste0(fulltext,collapse="")),"")[[1]]

```

# Get a 0-1-Matrix for a given Alphabet and Text.

```{r}
# Get a 0-1 Matrix for a given alphabet and text.


myletters <- c(letters, " ")

# function that looks up whether a letter from the alphabet is in x, gives back True or False 
where_in_alphabet_ <- function(x, alphabet = myletters) {alphabet %in% x}

# 0-1 Matrix with option to return True-False Matrix when setting numeric = FALSE 
where_in_alphabet <- function(x, alphabet = letters, numeric = TRUE){
  out <- t(apply(matrix(x), 1, where_in_alphabet_, alphabet = alphabet))
  colnames(out) <- alphabet
  rownames(out) <- x
  if (numeric) out <- out*1
  out
}

tic()
tfmatrix = where_in_alphabet(text, myletters)
toc()

head(tfmatrix)

```


```{r}



tic()
seqset = SequenceSet()
seq = Sequence()

ln <- ncol(tfmatrix)

for (i in 1:(min(nrow(tfmatrix)-1,80000))  ) {
  input_vals = tfmatrix[i,]
  target_vals = tfmatrix[i+1,]
  
 Sequence_add_from_array(seq, input_vals, target_vals, ln, ln)
 
 if (i %% 100==0) {
   SequenceSet_add_copy_of_sequence(seqset, seq)
   seq = Sequence()
 }
}
  
 
 toc()
```


```{r network}



TANH_NODE = 1

#hid_size = 40
#network = LSTMNetwork(ln,hid_size,ln)

network = NetworkFactory_createRecurrentWTANetwork(in_size=ln, hid_type=TANH_NODE, num_layers=1,
                                    layer_sizes=c(180),  out_size=ln);

Network_error_function_set(network, WinnerTakesAllErrorFunction())
```

## Auto-predict

A function to generate text from the network:

```{r autopred}

predict.text <- function(context=50, probabilistic=TRUE) {

sequence = SequenceSet_get(seqset,0)


 setClass("_p_std__vectorT_std__vectorT_double_std__allocatorT_double_t_t_p_std__allocatorT_std__vectorT_double_std__allocatorT_double_t_t_p_t_t", contains = 'ExternalReference')


if (!probabilistic) {
  predictor = AutoPredictor__SWIG_2(network, WinnerTakesAllTransferFunction() )
} else {
  predictor = AutoPredictor__SWIG_2(network, ProbabilisticWinnerTakesAllTransferFunction() )
}

time_steps <- 400
x = AutoPredictor_predict( predictor, sequence, time_steps, context)
vals <- sapply(1:time_steps, FUN = function(index){getRow(x, index-1)})
#char.ids <- apply(vals,2, which.max)


generate_text2 <- function(tfmatrix, alphabet = letters) {
  # browser()
  outputvector <- vector(mode = "character", length = nrow(tfmatrix))
  for (i in seq_len(NROW(tfmatrix))) {
    replace <- alphabet[which.max(tfmatrix[i, ])]
    if(length(replace) == 0L) replace <- NA_character_ #control for case of numbers, other non-alphabetic elements
    outputvector[i] <- replace
  }
  outputvector
}

paste0(generate_text2(t(vals), alphabet=myletters),collapse = "")

}
#x = Network_activate_and_return_activations(network, sequence)
```


Since the network only has random initial weights, the text will be random:

```{r}
predict.text()
```

Initialize Trainer and set learning rate:

```{r}
trainer = ImprovedRPropTrainer(network);
#trainer = ADAMTrainer(network)
Trainer_learning_rate_set( trainer, 0.01 )
```

Train the network

```{r training}
#Trainer_add_abort_criterion__SWIG_0(trainer, ConvergenceCriterion(0.1),1 )

tic()
Trainer_train2(trainer, seqset, 10)
toc()

#Network_save("/Users/brandmaier/Desktop/saved.nn")
```

Plot the traininer error

```{r}
	values <- .Call('R_swig_toValue',  Trainer_error_train_get(trainer), package="bnnlib") 
	
	library(ggplot2)
	ggplot2::ggplot(data.frame(x=1:length(values),values),aes(x=x,y=values))+geom_point()+
	  geom_line()+geom_smooth()
```

## Generate new sequences

```{r}
chop <- function(txt) { sapply(seq(1,nchar(txt),30),function(x){ substr(txt,x,x+30)})}
```

Generating new text from the network:


```{r}
predict.text(50)
```