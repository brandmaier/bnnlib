---
title: "Untitled"
author: "Andreas M. Brandmaier"
date: "11/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tictoc)
```


First, we load the shared library, some packages, and extra R code.

```{r fireup, echo=TRUE}
library(bnnlib)
library(gridExtra)
library(tictoc)
```

## The Bible

As training data, we use the English and German translation of the Bible as provided from the `qlcMatrix` package. 

```{r}
library(qlcMatrix)
data(bibles)

prepare_text <- function(x) {
 fulltext <- paste0(x,collapse = " ")
 text = strsplit(tolower(paste0(fulltext,collapse="")),"")[[1]]
 if (length(text)>80000)
   text <- text[1:80000] # take the first 80k letters
return(text)
}

text_eng <- prepare_text(bibles$eng)
text_ger <- prepare_text(bibles$deu)
```

Convert to 0/1 coding

```{r}
myletters <- c(letters, " ") # use English letters plus space

# function that looks up whether a letter from the alphabet is in x, gives back True or False 
where_in_alphabet_ <- function(x, alphabet = myletters) {alphabet %in% x}

# 0-1 Matrix with option to return True-False Matrix when setting numeric = FALSE 
where_in_alphabet <- function(x, alphabet = letters, numeric = TRUE){
  out <- t(apply(matrix(x), 1, where_in_alphabet_, alphabet = alphabet))
  colnames(out) <- alphabet
  rownames(out) <- x
  if (numeric) out <- out*1
  out
}

```

Convert into sequence set

```{r}

tic()
seqset = SequenceSet()

texts <- list(text_eng, text_ger)

for (j in 1:length(texts)) {
  
  cur_text <- texts[[j]]
  
  tfmatrix = where_in_alphabet(cur_text, myletters)

  seq = Sequence()

  ln <- ncol(tfmatrix)

  target_vals = c(0,0)
  target_vals[j]<-1

  for (i in 1:(min(nrow(tfmatrix)-1,80000))  ) {
   input_vals = tfmatrix[i,]

  
   Sequence_add_from_array(seq, input_vals, target_vals, ln, 2)
 
   if (i %% 1000==0) {
     #cat("New sequence ", i,"\n")
     SequenceSet_add_copy_of_sequence(seqset, seq)
     seq = Sequence()
   }
  }
  
}
 
 toc()
```

The following code creates a standard recurrent neural network with a winner-takes-all output layer. This layer enforces all outputs nodes to have activations larger or equal to zero while the sum of the activations equals 1. Thus, the output layer can be taken to represent a discrete probability distribution over letters.

```{r network}
network = NetworkFactory_createRecurrentWTANetwork(in_size=ln, hid_type=bnnlib::TANH_NODE, 
                                                   num_layers=1,
                                    layer_sizes=c(10,7),  out_size=2);

Network_error_function_set(network, WinnerTakesAllErrorFunction())
```

Initialize the improved resilient propagation trainer.

```{r}
trainer = ImprovedRPropTrainer(network);
```

Train the network. 

```{r training}

tic()
Trainer_train2(trainer, seqset, 40)
toc()

plotTrainingerror(trainer)

```

Investigate the network predictions within the training sample:

```{r}
seq1 <- SequenceSet_get(seqset, 1)
#plotPredictions(network = network, seq1 = seq1)

outs <- getOutputs(network, seq1)
plot(outs[,1],type="l") # plot the probability of the text being English
```

Now, let's predict a novel text fragment:

```{r}

novel_text <- strsplit("Dies ist ein Fragment in Deutscher Sprache. Wird das Netzwerk dies erkennen? Es bleibt spannend! Wir werden es gleich versuchen. Noch einen kleinen Moment Geduld bitte!","")[[1]]

novel_sequence <- add_text_to_sequenceset( novel_text, myletters, c(0,0))
outs <- getOutputs(network, novel_sequence)
plot(outs[,1],type="l",ylim=c(0,1), ylab="Probability",xlab="Letter") # plot probability of the text being English
lines(outs[,2],type="l",col="blue")
```
