---
title: "ImageCompression"
author: "Andreas M. Brandmaier"
date: "1/1/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(bnnlib)
library(png)
library(raster)
```

## Load Images

```{r cars}

images <- c("bottle1.png","bottle2.png","bottle3.png","fish1.png","fish2.png","fish3.png")

path<-"../OpenclipartShapes/"


sw<-function(pic)
{
	temp <- (pic[,,1]+pic[,,2]+pic[,,3])/3
	return(as.matrix(temp))
}

downsample<-function(pic, skip=16)
{
  pic[seq(1,1000,skip),seq(1,1000,skip)]
}


sequence_set <- SequenceSet()

for (fname in images) {
  img <- readPNG(paste0(path,fname))
  img <- downsample(sw(img))


  data.row <- as.vector(img)
  data.row <- data.frame(t(data.row)) # 1x 7056

dims <- length(img)

sequence <- toSequence(data.row, 1:dims, 1:dims)

SequenceSet_add_copy_of_sequence(sequence_set, sequence)
}
```

## Create auto-associator

```{r pressure, echo=FALSE}
dim <- sqrt(dims)
```

## Feedforward Neural Network

Create a feed-forward neural network with $84^2$ inputs, 4 hidden units, and $84^2$ output. 

```{r}
num_bottleneck = 10
sparsity = 0.5
net <- NetworkFactory_createTwoLayerSparseAutoencoder(dims,
                                              dim, num_bottleneck,
                                              sparsity, 
                                              bnnlib::RELU_NODE, 
                                              bnnlib::SIGMOID_NODE)
```


Initialize training algorithm. The sparsity $\beta$ value specifies the relative weight of the
sparsity beta in comparison to the loss function. The larger the beta, the more important sparsity is over the in-sample error.

```{r}
#trainer <- ImprovedRPropTrainer(net)
#trainer <- ADAMTrainer(net)
trainer <- BackpropTrainer(net)
#Trainer_set_mini_batch_size(trainer, 2)
Trainer_batch_learning_set(trainer, FALSE)
Trainer_learning_rate_set(trainer, 0.01)

 Network_evaluate_training_error__SWIG_0(net, sequence_set)
 
 
Trainer_sparsity_beta_set(trainer,0)
 
Sys.time()
Trainer_train__SWIG_0(trainer, sequence_set, 50)
Sys.time()
```

Inspect the training error over epochs:

```{r}
Network_evaluate_training_error__SWIG_0(net, sequence_set)


plot(plotTrainingerror(trainer)+scale_y_log10())
 
```

Obtain hidden unit activations for a given sequence:

```{r outputpredictedimage}

plotPreds <- function(sequence) {
x = getOutputs(net, sequence )
#x2 = sapply(1:(84*84), function(x){ Sequence_get_input_at(sequence,0,x-1)})
x2 = getInputs(sequence)
#getInputs()
predimg <- matrix(nrow=dim, ncol=dim)
inpimg <- matrix(nrow=dim, ncol=dim)
for (i in 1:dim) {
  predimg[i,] <- x[1:dim+(i-1)*dim]
  inpimg[i,] <- x2[1:dim+(i-1)*dim]
}

par(mfrow=c(2,2))
image(inpimg, main="Input")
image(predimg, main="Prediction")
image(predimg-inpimg, main="Difference")
image(abs(predimg-inpimg),main="Absolute Difference")
}

plotPreds( SequenceSet_get(sequence_set, 1) )
plotPreds( SequenceSet_get(sequence_set, 2) )
plotPreds( SequenceSet_get(sequence_set, 3) )

```

# Plot latent space projections

```{r}

pos <- which(sapply(getNodeNames(net) , function(x){startsWith(x,"Hidden")}))
#pos <- 7630:7670
result <- matrix(nrow=0,ncol=length(pos))
for (i in 1:SequenceSet_size(sequence_set)) {
  sequence <- SequenceSet_get(sequence_set,i-1)
  x = getActivations(net, sequence)[1,pos]
  result <- rbind(result, x)
}

result

if (sum(dist(result))!=0) {
 coords <- cmdscale(dist(result),2)
 plot(coords,type="n")
 text(x=coords[,1],y=coords[,2], labels=images)
}
```

# Inspect network weights

```{r}

e1 <- Network_get_ensemble(net, 0) # bias
e2 <- Network_get_ensemble(net, 1) # input layer
e3 <- Network_get_ensemble(net, 2) # hidden layer
e4 <- Network_get_ensemble(net, 3) # output layer

weights <- getOutgoingWeights(e1,e4)
hist(weights)

weights <- getOutgoingWeights(e3,e4)
hist(weights)
```