---
title: "Grammar Learning"
author: "Andreas M. Brandmaier"
date: "1/6/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a demonstration of a simple grammar learning tasks.  A formal grammar describes how to form strings from an alphabet that are valid according to the language's syntax. Here, we create a regular grammar (Type 3 in Chomskys hierarchy). Then, we randomly create sentences from that language and train a neural network to predict the next valid letter given a sentence. When the network successfully learns the regular language, it can be used as a generator of that language. In this simple demo, the language's alphabet has only three symbols `A`, `B`, and `C`.

## Setup

First, we load the shared library, some packages, and extra R code.

```{r fireup, echo=TRUE, message=FALSE}
library(bnnlib)
library(tictoc)
```


## Generate Data

Generate data from a simple state machine.
Valid transitions are:

- from A to B
- from B to A or C
- from C to CB (always two Cs then B)

This means there can never be a transition from A to C or from C to A. Also, an A can never be followed by another A and a B can never be followed by another B.

```{r generator}
set.seed(3245)
total.size <- 4000
text <- rep("A",total.size)
for (i in 2:total.size) {
  if (text[i-1]=="A") { text[i]="B"}
  if (text[i-1]=="B") {
    if (runif(1)<.5) text[i]="A"
    else text[i]="C"
  }
  if (text[i-1]=="C") {
    if (text[i-2]=="C")
     text[i]="B"
    else
     text[i]="C"
  }
}

# compute the number of "B"s and divide by two to
# obtain lowest possible error
lowest.mean.error <- sum(text=="B")/2
```

We generate a sentence of length `r total.size`. Here is the beginning of the sequence

```{r}
cat(text[1:10])
```

# Recode sentences

In the following, we recode the generated sequences from the formal language into a one-hot-coding, in which there is one column for each letter and in each row, there is always only one value set to '1' to represent the presence of the corresponding character whereas all other columns have the value '0'.

```{r}
# Get a 0-1 Matrix for a given alphabet and text.

myletters <- c("A","B","C")
tfmatrix = onehot(text, myletters)
head(tfmatrix)

```


```{r}

tic()
seqset = SequenceSet()
seq = Sequence()

ln <- ncol(tfmatrix)

for (i in 1:(min(nrow(tfmatrix)-1,40000))  ) {
  input_vals = tfmatrix[i,]
  target_vals = tfmatrix[i+1,]
  
 Sequence_add_from_array(seq, input_vals, target_vals, ln, ln)
}
  
 SequenceSet_add_copy_of_sequence(seqset, seq)
 toc()
```


```{r network}

#hid_size = 4
#network = LSTMNetwork(ln,hid_size,ln)

network = NetworkFactory_createRecurrentWTANetwork(in_size=ln, hid_type=bnnlib::TANH_NODE, 
                                                   num_layers=2,
                                    layer_sizes=c(10,5),  out_size=ln);

```

## Auto-predict

A function to generate text from the network:

```{r autopred}

predict.text <- function(context=20, probabilistic=TRUE) {

sequence = SequenceSet_get(seqset,0)

if (!probabilistic) {
  predictor = AutoPredictor__SWIG_2(network, WinnerTakesAllTransferFunction() )
} else {
  predictor = AutoPredictor__SWIG_2(network, ProbabilisticWinnerTakesAllTransferFunction() )
}

time_steps <- 400
x = AutoPredictor_predict( predictor, sequence, time_steps, context)
vals <- sapply(1:time_steps, FUN = function(index){getRow(x, index-1)})


generate_text2 <- function(tfmatrix, alphabet = letters) {
  outputvector <- vector(mode = "character", length = nrow(tfmatrix))
  for (i in seq_len(NROW(tfmatrix))) {
    replace <- alphabet[which.max(tfmatrix[i, ])]
    if(length(replace) == 0L) replace <- NA_character_ #control for case of numbers, other non-alphabetic elements
    outputvector[i] <- replace
  }
  outputvector
}

paste0(generate_text2(t(vals), alphabet=myletters),collapse = "")

}
```


Since the network only has random initial weights, the text will be random (or often, constant):

```{r}
predict.text()
```

Initialize Trainer and set learning rate:

```{r trainer}
trainer = ImprovedRPropTrainer(network);
Trainer_learning_rate_set( trainer, 0.01 )
trainer = ADAMTrainer(network);

```

Train the network and take time.

```{r training}
Trainer_add_abort_criterion__SWIG_0(trainer, ConvergenceCriterion(0.001),1 )

tic()
Trainer_train2(trainer, seqset, 500)
toc()

```

Plot the training error.

```{r}

source("../R/plotTrainingerror.R")
plotTrainingerror(trainer) + theme_light() +
  geom_hline(yintercept=lowest.mean.error,lty=2)
```

## Generate new sequences

```{r}
chop <- function(txt) { sapply(seq(1,nchar(txt),30),function(x){ substr(txt,x,x+30)})}
```

Generating new text from the network using probabilistic mapping (that is, the next value is randomly chosen according to the distribution represented in the output layer):

```{r}
chop(predict.text(1))
```

Generating new text from network using MAP mapping (we always choose the symbol that has the largest likelihood):

```{r}
chop(predict.text(1,FALSE))
```

## Inspect Predictions more closely

First, we get all outputs from the network.

```{r}
outputs <- getOutputs(network, seq)
```

Let's see what the network predicts after having seen the first input from the sequence `r text[1]`.

```{r}
barplot(outputs[1,], names.arg = myletters)
```

and next after having seen `r (text[2])`.

```{r}
barplot(outputs[2,], names.arg = myletters)
```

and next after having seen `r (text[3])`.

```{r}
barplot(outputs[3,], names.arg = myletters)
```

Inspect predictions and input. The rows show the true letter and the columns show the predicted letter.
```{r}
predicted.text <- c("A","B","C")[apply(getOutputs(network, seq),1, which.max)]
table(text[-1],predicted.text)
````

