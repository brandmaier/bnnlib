---
title: "Grammar Learning"
author: "Andreas M. Brandmaier"
date: "1/6/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a demonstration of a simple grammar learning tasks.  A formal grammar describes how to form strings from an alphabet that are valid according to the language's syntax. Here, we create a regular grammar (Type 3 in Chomskys hierarchy). Then, we randomly create sentences from that language and train a neural network to predict the next valid letter given a sentence. When the network successfully learns the regular language, it can be used as a generator of that language. In this simple demo, the language's alphabet has only three symbols `A`, `B`, and `C`.

## Setup

First, we load the shared library, some packages, and extra R code.

```{r fireup, echo=TRUE}

dyn.load(paste("../bnnlib", .Platform$dynlib.ext, sep=""))
source("../bnnlib.R")
cacheMetaData(1)

library(gridExtra)
library(tictoc)

source("../R/toSequence.R")
```


## Generate Data

Generate data from a simple state machine.
Transitions are:
- from A to B
- from B to A or C
- from C to CB (always two Cs then B)

```{r generator}
total.size <- 4000
text <- rep("A",total.size)
for (i in 2:total.size) {
  if (text[i-1]=="A") { text[i]="B"}
  if (text[i-1]=="B") {
    if (runif(1)<.5) text[i]="A"
    else text[i]="C"
  }
  if (text[i-1]=="C") {
    if (text[i-2]=="C")
     text[i]="B"
    else
     text[i]="C"
  }
}


```

We generate a sentence of length `r total.size`.

# Recode sentences

In the following, we recode the generated sequences from the formal language into a one-hot-coding, in which there is one column for each letter and in each row, there is always only one value set to '1' to represent the presence of the corresponding character whereas all other columns have the value '0'.

```{r}
# Get a 0-1 Matrix for a given alphabet and text.
source("../R/onehot.R")

myletters <- c("A","B","C")
tfmatrix = onehot(text, myletters)


head(tfmatrix)

```


```{r}

tic()
seqset = SequenceSet()
seq = Sequence()

ln <- ncol(tfmatrix)

for (i in 1:(min(nrow(tfmatrix)-1,40000))  ) {
  input_vals = tfmatrix[i,]
  target_vals = tfmatrix[i+1,]
  
 Sequence_add_from_array(seq, input_vals, target_vals, ln, ln)
}
  
 SequenceSet_add_copy_of_sequence(seqset, seq)
 toc()
```


```{r network}



TANH_NODE = 1

#hid_size = 4
#network = LSTMNetwork(ln,hid_size,ln)

network = NetworkFactory_createRecurrentWTANetwork(in_size=ln, hid_type=TANH_NODE, num_layers=2,
                                    layer_sizes=c(5,5),  out_size=ln);

```

## Auto-predict

A function to generate text from the network:

```{r autopred}

predict.text <- function(context=20, probabilistic=TRUE) {

sequence = SequenceSet_get(seqset,0)


 setClass("_p_std__vectorT_std__vectorT_double_std__allocatorT_double_t_t_p_std__allocatorT_std__vectorT_double_std__allocatorT_double_t_t_p_t_t", contains = 'ExternalReference')


if (!probabilistic) {
  predictor = AutoPredictor__SWIG_2(network, WinnerTakesAllTransferFunction() )
} else {
  predictor = AutoPredictor__SWIG_2(network, ProbabilisticWinnerTakesAllTransferFunction() )
}

time_steps <- 400
x = AutoPredictor_predict( predictor, sequence, time_steps, context)
vals <- sapply(1:time_steps, FUN = function(index){getRow(x, index-1)})
#char.ids <- apply(vals,2, which.max)


generate_text2 <- function(tfmatrix, alphabet = letters) {
  # browser()
  outputvector <- vector(mode = "character", length = nrow(tfmatrix))
  for (i in seq_len(NROW(tfmatrix))) {
    replace <- alphabet[which.max(tfmatrix[i, ])]
    if(length(replace) == 0L) replace <- NA_character_ #control for case of numbers, other non-alphabetic elements
    outputvector[i] <- replace
  }
  outputvector
}

paste0(generate_text2(t(vals), alphabet=myletters),collapse = "")

}
#x = Network_activate_and_return_activations(network, sequence)
```


Since the network only has random initial weights, the text will be random:

```{r}
predict.text()
```

Initialize Trainer and set learning rate:

```{r trainer}
trainer = ImprovedRPropTrainer(network);
Trainer_learning_rate_set( trainer, 0.01 )
```

Train the network

```{r training}
Trainer_add_abort_criterion__SWIG_0(trainer, ConvergenceCriterion(0.001),1 )

tic()
Trainer_train2(trainer, seqset, 500)
toc()

source("../R/plotTrainingerror.R")
plotTrainingerror(trainer) + theme_light()
```

## Generate new sequences

```{r}
chop <- function(txt) { sapply(seq(1,nchar(txt),30),function(x){ substr(txt,x,x+30)})}
```

Generating new text from the network using probability mapping:

```{r}
chop(predict.text(1))
```

Generating new text from network using MAP mapping:
```{r}
chop(predict.text(1,FALSE))
```
