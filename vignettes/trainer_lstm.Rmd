---
title: "Comparing Different Training Algorithms"
author: "Andreas M. Brandmaier"
date: "12/12/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE)
```

## Introduction

In this document, we will compare the convergence of different training algorithms for a given dataset and network architecture. The data will be simulated from a simple toy problem, in which there are four i.i.d. Gaussian predictors and a single metric outcome. 

## Load Library

First, we load the shared library and some internal functions (which will soon be integrated into the package).

```{r pressure, echo=TRUE}


  dyn.load(paste("../bnnlib", .Platform$dynlib.ext, sep=""))
  source("../bnnlib.R")
  cacheMetaData(1)
  
  source("../R/toSequence.R")
  source("../R/plot.io.R")
  source("../R/zzz.R")

```

## Generate Data

Next, we simulate some data in a `data.frame` and convert this to bnn's `Sequence` format. 

```{r}
set.seed(34235) # for R code
setRandomSeed(335) # for c++ code


seq <- SequenceSet()
test <- SequenceSet()
delay <- 30

num.seq <- 10

for (i in 1:num.seq) {
len <- 50
# create a sequence from scratch
input <- rep(0,len)
output <- rep(0,len)

pos <- sample(1:(len-delay-1),1)
input[pos]<-1
output[pos+delay]<-1

seq1<-Sequence(input,output,len)

if (i > num.seq/2) {
  SequenceSet_add_copy_of_sequence(test, seq1)
} else {
  SequenceSet_add_copy_of_sequence(seq,seq1)
}
  
}

```

Each of the sequences has a length of `r len` and we created `r num.seq` many sequences. Half of them are in the SequenceSet called `test` and the other half in the SequenceSet called `seq`. This is how the sequences look like:

```{r}

library(ggplot2)
plot.io(SequenceSet_get(seq, 0))
```

## Create Network

Now, we create a feed-forward network with a single hidden layer of 10 neurons. 

```{r}
net <- LSTMNetwork(1,4,1)

```

## Trainer

Let's generate a list of different training algorithms and run them each for 100 steps. Save the results in `err.data`:

```{r}
bp <- BackpropTrainer(net)
Trainer_batch_learning_set(bp, FALSE)

trainer <- list( RMSPropTrainer(net), ADAMTrainer(net),
                 BackpropTrainer(net), bp, 
                 RPropTrainer(net),
                 ARPropTrainer(net),
                  ImprovedRPropTrainer(net),
                 ZZPropTrainer(net))

steps <- 280
err.data <- matrix(data=NA,nrow=steps,ncol=length(trainer))
err.data <- data.frame(err.data)
names(err.data) <- c("error")
elapsed.time <- list()



for (i in 1:length(trainer)) {
  bp <- trainer[[i]]
  
  #Trainer_weight_decay_set(bp, 0.05)
  #Trainer_weight_decay_enabled_set(bp, TRUE)

  Network_reinitialise(net)

    last.timestamp <- proc.time()
  
    Trainer_train__SWIG_0(bp, seq, steps)
    
    elapsed.time[[i]] <- proc.time()-last.timestamp
    last.timestamp <- proc.time()
    
    x<-Trainer_error_train_get(bp)
    err.data[,i] <- .Call('R_swig_toValue',  x, package="bnnlib") 
    colnames(err.data)[i] <- paste0(i,":", Trainer_get_name(bp))
    
}

err.data$time <- 1:steps

runtimes <- sapply(elapsed.time, function(x){round(x[3],2)})
names(runtimes) <- colnames(err.data[1:length(trainer)])

```

Plot the runtimes:
```{r}
ggplot(reshape2::melt(as.matrix(runtimes)),aes(y=value,x=Var1,color=Var1))+
  geom_bar(stat="identity",fill="white")+xlab("Trainers")+ylab("Time [s]")+
  geom_text(aes(label=value), vjust=1.6, color="black", size=3.5)+
  theme_minimal()+ theme(legend.position="none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Plot the errors

```{r}
library(ggplot2)

err.data.long <- reshape2::melt(err.data,id.vars="time")

ggplot(data=err.data.long, 
       aes(x=time,y=value,group=variable,col=variable))+
  geom_point()+
  geom_line()+
  theme_minimal()+
  ylim(0,10)
```


# Test set predictions

To test the generalization performance of the network, we take a `Sequence` previously unseen by the network and feed it to the network. 

```{r}
source("../R/toSequence.R")
s1 = SequenceSet_get(test, 0)
input = getInputs(s1)
target <- getTargets(s1)
spike.position = which.max(getInputs(s1))
```

The position of the input spike is at time point `r spike.position`, that is, we expect the network to generate an output spike after a delay of `r delay` on time point `r spike.position+delay`. 

```{r fig.cap="This is the figure caption."}
output = getOutputs(net, s1)
ln <- length(output)
longdat <- reshape2::melt(data.frame(id=1:ln,target,output),id.vars="id")
ggplot(data=longdat, aes(x=id,y=value,group=variable,col=variable))+geom_line()+
  theme_minimal()+xlab("Time")+ylab("Activation")
```